**Web scraping**
---
El web scraping (o raspado web) es una tÃ©cnica utilizada para extraer informaciÃ³n de sitios web. Consiste en utilizar programas o scripts para automatizar la navegaciÃ³n por un sitio web y recolectar informaciÃ³n de manera automatizada. Los datos recolectados pueden ser utilizados para diversos fines, como la monitorizaciÃ³n de precios, la recopilaciÃ³n de datos para anÃ¡lisis, la generaciÃ³n de contenido automatizado, entre otros. Algunos ejemplos de uso del web scraping incluyen el anÃ¡lisis de tendencias en redes sociales, la extracciÃ³n de datos de sitios de comparaciÃ³n de precios, la recopilaciÃ³n de noticias y actualizaciones de precios en sitios de comercio electrÃ³nico.

**Pupeteer y Mongoose**
---
**Puppeteer** es una biblioteca de Node.js desarrollada por Google que permite automatizar tareas en un navegador Chrome o Chromium. Con Puppeteer, se pueden crear scripts para realizar tareas como navegar por sitios web, rellenar formularios, hacer clic en botones, tomar capturas de pantalla y extraer datos de pÃ¡ginas web. Puppeteer tambiÃ©n proporciona una interfaz para interactuar con una pÃ¡gina web utilizando JavaScript, lo que permite realizar tareas avanzadas como simular eventos del usuario, ejecutar JavaScript en una pÃ¡gina y acceder a elementos de la pÃ¡gina que no estÃ¡n disponibles para el usuario final. Es importante mencionar que Puppeteer es una herramienta de automatizaciÃ³n de navegador y no una librerÃ­a de web scraping, pero puede ser utilizada para realizar tareas de scraping debido a su capacidad para interactuar con la pÃ¡gina web y extraer informaciÃ³n.

**Mongoose** es una biblioteca de Node.js que proporciona una interfaz de alto nivel para interactuar con una base de datos MongoDB. Mongoose proporciona una serie de caracterÃ­sticas para trabajar con MongoDB, como la definiciÃ³n de esquemas para los documentos, la validaciÃ³n de datos, la construcciÃ³n de consultas y la conexiÃ³n a una base de datos.

**InstalaciÃ³n**
---

Inicializar un paquete de npm

```jsx
npm init -y
```

InstalaciÃ³n de los paquetes necesarios

```jsx
npm i pupeteer mongoose
```

crear un fichero index.js

```jsx
touch index.js
```

aÃ±adir el script para ejecutar dicho fichero en el package.json

```jsx
"start": node index.js
```

**GuÃ­a**
---
En nuestro fichero index.js

- Se importan las bibliotecas de Puppeteer y Mongoose al principio del script.
- Se crea un modelo de datos llamado "Data" utilizando Mongoose, con dos campos: tÃ­tulo y precio.
- Se define una funciÃ³n "connect" para conectarse a la base de datos MongoDB.
- Se define una funciÃ³n "scrapeProducts" para extraer los datos de la pÃ¡gina web.
    
    a. Se llama a la funciÃ³n "connect" para conectarse a la base de datos.
    b. Se inicializa el navegador con Puppeteer y se abre una nueva pestaÃ±a.
    c. Se navega a la URL especÃ­fica (Amazon en este caso) y se realiza una bÃºsqueda en el sitio utilizando la palabra "star wars".
    d. Se espera a que la pÃ¡gina se cargue y se hace un scroll para ver mÃ¡s resultados de bÃºsqueda.
    e. Utilizando el mÃ©todo ".$$eval" de Puppeteer se extraen los tÃ­tulos y precios de los productos de la pÃ¡gina.
    f. Se crea un array con los datos extraÃ­dos y se convierte en un objeto JSON.
    g. Se guarda el objeto JSON en la base de datos MongoDB.
    h. Se cierra el navegador y se imprime un mensaje de Ã©xito.
    
- Finalmente, se llama a la funciÃ³n "scrapeProducts" para iniciar el proceso de extracciÃ³n y guardado de datos.


**Code**
---
```jsx
const puppeteer = require('puppeteer')
const mongoose = require('mongoose')

const Data = mongoose.model('Data', new mongoose.Schema({
  title: String,
  price: String
}))

const connect = async () => {
  try {
    const URI = 'mongodb+srv://user:password@cluster0.qdiqj9n.mongodb.net/nameProject?retryWrites=true&w=majority'
    await mongoose.connect(URI, { useNewUrlParser: true, useUnifiedTopology: true })
    console.log('Connected to DB ğŸš€')
  } catch (error) {
    console.error('Not connected to DB âŒ')
  }
}

const scrapeProducts = async () => {
  await connect()

  const url = 'https://www.amazon.es/'

  const browser = await puppeteer.launch({
    headless: false,
    defaultViewport: null,
    args: ['--start-maximized']
  })

  const page = await browser.newPage()

  await page.goto(url)

  await page.type('#twotabsearchtextbox', 'star wars')

  await page.click('#nav-search-submit-button')

  await page.waitForSelector('.s-pagination-next')

  const title = await page.$$eval('h2 span.a-color-base', (nodes) =>
    nodes.map((n) => n.innerText)
  )

  const price = await page.$$eval('span.a-price[data-a-color="base"] span.a-offscreen', (nodes) =>
    nodes.map((n) => n.innerText)
  )

  const amazonProduct = title.map((value, index) => {
    return {
      title: title[index],
      price: price[index]
    }
  })

  amazonProduct.map(async (data) => {
    const dataSchema = new Data(data)
    try {
      await dataSchema.save()
      console.log(`Succesfully saved ${dataSchema.title} to the database ğŸ¤˜ğŸ½`)
    } catch (error) {
      console.error(`Failed to save ${dataSchema.title} to the database âŒ`)
    }
  })

  await browser.close()
  console.log('All saved Succesfully - Live Rock ğŸ¤˜ğŸ½ğŸ§™ğŸ½â€â™‚ï¸')

}

scrapeProducts()
```